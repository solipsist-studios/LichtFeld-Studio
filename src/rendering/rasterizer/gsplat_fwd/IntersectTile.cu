/* SPDX-FileCopyrightText: 2025 LichtFeld Studio Authors
 *
 * SPDX-License-Identifier: GPL-3.0-or-later */

#include <cmath>
#include <cooperative_groups.h>
#include <cstdio>
#include <cub/cub.cuh>
#include <cuda_runtime.h>
#include <thrust/iterator/transform_iterator.h>

#include "Common.h"
#include "Intersect.h"
#include "Utils.cuh"

namespace gsplat_fwd {

    namespace cg = cooperative_groups;

    template <typename scalar_t, bool kWrapX>
    __global__ void intersect_tile_kernel(
        const bool packed,
        const uint32_t C,
        const uint32_t N,
        const uint32_t nnz,
        const int64_t* __restrict__ camera_ids,
        const int64_t* __restrict__ gaussian_ids,
        const scalar_t* __restrict__ means2d,
        const int32_t* __restrict__ radii,
        const scalar_t* __restrict__ depths,
        const int64_t* __restrict__ cum_tiles_per_gauss,
        const uint32_t tile_size,
        const uint32_t tile_width,
        const uint32_t tile_height,
        const uint32_t tile_n_bits,
        int32_t* __restrict__ tiles_per_gauss,
        int64_t* __restrict__ isect_ids,
        int32_t* __restrict__ flatten_ids) {
        uint32_t idx = cg::this_grid().thread_rank();
        bool first_pass = cum_tiles_per_gauss == nullptr;
        if (idx >= (packed ? nnz : C * N)) {
            return;
        }

        const float radius_x = radii[idx * 2];
        const float radius_y = radii[idx * 2 + 1];
        if (radius_x <= 0 || radius_y <= 0) {
            if (first_pass) {
                tiles_per_gauss[idx] = 0;
            }
            return;
        }

        vec2 mean2d = glm::make_vec2(means2d + 2 * idx);

        float tile_radius_x = radius_x / static_cast<float>(tile_size);
        float tile_radius_y = radius_y / static_cast<float>(tile_size);
        float tile_x = mean2d.x / static_cast<float>(tile_size);
        float tile_y = mean2d.y / static_cast<float>(tile_size);

        const int32_t tile_width_i = static_cast<int32_t>(tile_width);
        const int32_t tile_height_i = static_cast<int32_t>(tile_height);

        const int32_t tile_min_y = min(max(0, static_cast<int32_t>(floorf(tile_y - tile_radius_y))), tile_height_i);
        const int32_t tile_max_y = min(max(0, static_cast<int32_t>(ceilf(tile_y + tile_radius_y))), tile_height_i);

        if (tile_min_y >= tile_max_y) {
            if (first_pass) {
                tiles_per_gauss[idx] = 0;
            }
            return;
        }

        int32_t x0 = static_cast<int32_t>(floorf(tile_x - tile_radius_x));
        int32_t x1 = static_cast<int32_t>(ceilf(tile_x + tile_radius_x)); // exclusive
        int32_t span_x = x1 - x0;

        // Represent X coverage as the union of up to two non-overlapping intervals.
        int32_t interval0_start = 0, interval0_end = 0;
        int32_t interval1_start = 0, interval1_end = 0;

        if constexpr (kWrapX) {
            if (tile_width_i > 0) {
                if (span_x >= tile_width_i) {
                    // Covers full horizontal domain.
                    interval0_start = 0;
                    interval0_end = tile_width_i;
                } else if (span_x > 0) {
                    int32_t start = x0 % tile_width_i;
                    if (start < 0) {
                        start += tile_width_i;
                    }
                    int32_t end = start + span_x;
                    if (end <= tile_width_i) {
                        interval0_start = start;
                        interval0_end = end;
                    } else {
                        interval0_start = start;
                        interval0_end = tile_width_i;
                        interval1_start = 0;
                        interval1_end = end - tile_width_i;
                    }
                }
            }
        } else {
            x0 = min(max(0, x0), tile_width_i);
            x1 = min(max(0, x1), tile_width_i);
            if (x1 > x0) {
                interval0_start = x0;
                interval0_end = x1;
            }
        }

        const int32_t tile_count_x =
            (interval0_end - interval0_start) + (interval1_end - interval1_start);

        if (tile_count_x <= 0) {
            if (first_pass) {
                tiles_per_gauss[idx] = 0;
            }
            return;
        }

        if (first_pass) {
            tiles_per_gauss[idx] = static_cast<int32_t>(
                (tile_max_y - tile_min_y) * tile_count_x);
            return;
        }

        int64_t cid;
        if (packed) {
            cid = camera_ids[idx];
        } else {
            cid = idx / N;
        }
        const int64_t cid_enc = cid << (32 + tile_n_bits);

        int32_t depth_i32 = *(int32_t*)&(depths[idx]);
        int64_t depth_id_enc = static_cast<uint32_t>(depth_i32);

        int64_t cur_idx = (idx == 0) ? 0 : cum_tiles_per_gauss[idx - 1];
        for (int32_t i = tile_min_y; i < tile_max_y; ++i) {
            for (int32_t j = interval0_start; j < interval0_end; ++j) {
                int64_t tile_id = i * tile_width + j;
                isect_ids[cur_idx] = cid_enc | (tile_id << 32) | depth_id_enc;
                flatten_ids[cur_idx] = static_cast<int32_t>(idx);
                ++cur_idx;
            }
            for (int32_t j = interval1_start; j < interval1_end; ++j) {
                int64_t tile_id = i * tile_width + j;
                isect_ids[cur_idx] = cid_enc | (tile_id << 32) | depth_id_enc;
                flatten_ids[cur_idx] = static_cast<int32_t>(idx);
                ++cur_idx;
            }
        }
    }

    void launch_intersect_tile_kernel(
        const float* means2d,
        const int32_t* radii,
        const float* depths,
        const int64_t* camera_ids,
        const int64_t* gaussian_ids,
        uint32_t C,
        uint32_t N,
        uint32_t nnz,
        bool packed,
        uint32_t tile_size,
        uint32_t tile_width,
        uint32_t tile_height,
        bool wrap_x,
        const int64_t* cum_tiles_per_gauss,
        int32_t* tiles_per_gauss,
        int64_t* isect_ids,
        int32_t* flatten_ids,
        cudaStream_t stream) {
        int64_t n_elements = packed ? nnz : C * N;

        uint32_t n_tiles = tile_width * tile_height;
        uint32_t tile_n_bits = static_cast<uint32_t>(floor(log2(n_tiles))) + 1;

        if (n_elements == 0) {
            return;
        }

        dim3 threads(256);
        dim3 grid((n_elements + threads.x - 1) / threads.x);

        if (wrap_x) {
            intersect_tile_kernel<float, true><<<grid, threads, 0, stream>>>(
                packed,
                C, N, nnz,
                camera_ids, gaussian_ids,
                means2d, radii, depths,
                cum_tiles_per_gauss,
                tile_size, tile_width, tile_height, tile_n_bits,
                tiles_per_gauss, isect_ids, flatten_ids);
        } else {
            intersect_tile_kernel<float, false><<<grid, threads, 0, stream>>>(
                packed,
                C, N, nnz,
                camera_ids, gaussian_ids,
                means2d, radii, depths,
                cum_tiles_per_gauss,
                tile_size, tile_width, tile_height, tile_n_bits,
                tiles_per_gauss, isect_ids, flatten_ids);
        }
    }

    __global__ void intersect_offset_kernel(
        const uint32_t n_isects,
        const int64_t* __restrict__ isect_ids,
        const uint32_t C,
        const uint32_t n_tiles,
        const uint32_t tile_n_bits,
        int32_t* __restrict__ offsets) {
        uint32_t idx = cg::this_grid().thread_rank();
        if (idx >= n_isects)
            return;

        int64_t isect_id_curr = isect_ids[idx] >> 32;
        int64_t cid_curr = isect_id_curr >> tile_n_bits;
        int64_t tid_curr = isect_id_curr & ((1 << tile_n_bits) - 1);
        int64_t id_curr = cid_curr * n_tiles + tid_curr;

        if (idx == 0) {
            for (uint32_t i = 0; i < id_curr + 1; ++i)
                offsets[i] = static_cast<int32_t>(idx);
        }
        if (idx == n_isects - 1) {
            for (uint32_t i = id_curr + 1; i < C * n_tiles; ++i)
                offsets[i] = static_cast<int32_t>(n_isects);
        }

        if (idx > 0) {
            int64_t isect_id_prev = isect_ids[idx - 1] >> 32;
            if (isect_id_prev == isect_id_curr)
                return;

            int64_t cid_prev = isect_id_prev >> tile_n_bits;
            int64_t tid_prev = isect_id_prev & ((1 << tile_n_bits) - 1);
            int64_t id_prev = cid_prev * n_tiles + tid_prev;
            for (uint32_t i = id_prev + 1; i < id_curr + 1; ++i)
                offsets[i] = static_cast<int32_t>(idx);
        }
    }

    void launch_intersect_offset_kernel(
        const int64_t* isect_ids,
        uint32_t n_isects,
        uint32_t C,
        uint32_t tile_width,
        uint32_t tile_height,
        int32_t* offsets,
        cudaStream_t stream) {
        if (n_isects == 0) {
            cudaMemsetAsync(offsets, 0, C * tile_height * tile_width * sizeof(int32_t), stream);
            return;
        }

        dim3 threads(256);
        dim3 grid((n_isects + threads.x - 1) / threads.x);

        uint32_t n_tiles = tile_width * tile_height;
        uint32_t tile_n_bits = static_cast<uint32_t>(floor(log2(n_tiles))) + 1;

        intersect_offset_kernel<<<grid, threads, 0, stream>>>(
            n_isects, isect_ids, C, n_tiles, tile_n_bits, offsets);
    }

    void radix_sort_double_buffer(
        int64_t n_isects,
        uint32_t tile_n_bits,
        uint32_t cam_n_bits,
        int64_t* isect_ids,
        int32_t* flatten_ids,
        int64_t* isect_ids_sorted,
        int32_t* flatten_ids_sorted,
        cudaStream_t stream) {
        if (n_isects <= 0) {
            return;
        }

        cub::DoubleBuffer<int64_t> d_keys(isect_ids, isect_ids_sorted);
        cub::DoubleBuffer<int32_t> d_values(flatten_ids, flatten_ids_sorted);

        CUB_WRAPPER_LFS(
            cub::DeviceRadixSort::SortPairs,
            d_keys,
            d_values,
            n_isects,
            0,
            32 + tile_n_bits + cam_n_bits,
            stream);

        // Copy results to sorted buffers if needed
        if (d_keys.selector == 0) {
            cudaMemcpyAsync(isect_ids_sorted, isect_ids,
                            n_isects * sizeof(int64_t), cudaMemcpyDeviceToDevice, stream);
        }
        if (d_values.selector == 0) {
            cudaMemcpyAsync(flatten_ids_sorted, flatten_ids,
                            n_isects * sizeof(int32_t), cudaMemcpyDeviceToDevice, stream);
        }
    }

    void compute_cumsum_gpu(
        const int32_t* input,
        int64_t* output,
        uint32_t n_elements,
        cudaStream_t stream) {
        if (n_elements == 0) {
            return;
        }

        auto cast_op = [] __host__ __device__(int32_t x) { return static_cast<int64_t>(x); };
        auto cast_iter = thrust::make_transform_iterator(input, cast_op);

        CUB_WRAPPER_LFS(
            cub::DeviceScan::InclusiveSum,
            cast_iter,
            output,
            n_elements,
            stream);
    }

} // namespace gsplat_fwd
