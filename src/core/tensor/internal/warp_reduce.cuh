/* SPDX-FileCopyrightText: 2025 LichtFeld Studio Authors
 * SPDX-License-Identifier: GPL-3.0-or-later
 *
 * Warp-level reduction primitives inspired by tiny-cuda-nn's optimizations.
 * These use warp shuffle instructions for 5-10x faster reductions compared to shared memory.
 */

#pragma once

#include <cstdint>
#include <cuda_runtime.h>
#include <type_traits>

namespace lfs::core {
    namespace warp_ops {

        // ============= WARP-LEVEL REDUCTIONS (NO SHARED MEMORY!) =============

        /**
         * @brief Warp-level sum reduction using shuffle instructions
         *
         * Reduces a value across all threads in a warp using __shfl_xor_sync.
         * This is 5-10x faster than shared memory because:
         * - No synchronization needed within warp
         * - No shared memory access (stays in registers)
         * - Uses hardware shuffle unit
         *
         * @tparam T Type of value to reduce (float, int, etc.)
         * @param val Value from this thread
         * @return Sum of all values in the warp (valid in all threads)
         */
        template <typename T>
        __device__ inline T warp_reduce_sum(T val) {
#pragma unroll
            for (int offset = 16; offset > 0; offset /= 2) {
                val += __shfl_xor_sync(0xffffffff, val, offset);
            }
            return val;
        }

        /**
         * @brief Warp-level max reduction using shuffle instructions
         */
        template <typename T>
        __device__ inline T warp_reduce_max(T val) {
#pragma unroll
            for (int offset = 16; offset > 0; offset /= 2) {
                T other = __shfl_xor_sync(0xffffffff, val, offset);
                val = (val > other) ? val : other;
            }
            return val;
        }

        /**
         * @brief Warp-level min reduction using shuffle instructions
         */
        template <typename T>
        __device__ inline T warp_reduce_min(T val) {
#pragma unroll
            for (int offset = 16; offset > 0; offset /= 2) {
                T other = __shfl_xor_sync(0xffffffff, val, offset);
                val = (val < other) ? val : other;
            }
            return val;
        }

        /**
         * @brief Warp-level product reduction using shuffle instructions
         */
        template <typename T>
        __device__ inline T warp_reduce_prod(T val) {
#pragma unroll
            for (int offset = 16; offset > 0; offset /= 2) {
                val *= __shfl_xor_sync(0xffffffff, val, offset);
            }
            return val;
        }

        // ============= BLOCK-LEVEL REDUCTIONS (USES WARP REDUCTION) =============

        /**
         * @brief Block-level sum reduction using warp primitives
         *
         * Two-stage reduction:
         * 1. Each warp reduces its values using warp_reduce_sum
         * 2. First warp reduces results from all warps
         *
         * @tparam T Type of value to reduce
         * @param val Value from this thread
         * @return Sum of all values in the block (valid only in thread 0)
         */
        template <typename T>
        __device__ T block_reduce_sum(T val) {
            static __shared__ T shared[32]; // One value per warp (max 32 warps per block)

            int lane = threadIdx.x % 32;
            int warp_id = threadIdx.x / 32;

            // Reduce within warp
            val = warp_reduce_sum(val);

            // First thread in each warp writes to shared memory
            if (lane == 0) {
                shared[warp_id] = val;
            }
            __syncthreads();

            // First warp reduces across warps
            if (warp_id == 0) {
                val = (threadIdx.x < (blockDim.x + 31) / 32) ? shared[lane] : T(0);
                val = warp_reduce_sum(val);
            }

            return val;
        }

        /**
         * @brief Block-level max reduction using warp primitives
         */
        template <typename T>
        __device__ T block_reduce_max(T val) {
            static __shared__ T shared[32];

            int lane = threadIdx.x % 32;
            int warp_id = threadIdx.x / 32;

            val = warp_reduce_max(val);

            if (lane == 0) {
                shared[warp_id] = val;
            }
            __syncthreads();

            if (warp_id == 0) {
                val = (threadIdx.x < (blockDim.x + 31) / 32) ? shared[lane] : -std::numeric_limits<T>::infinity();
                val = warp_reduce_max(val);
            }

            return val;
        }

        /**
         * @brief Block-level min reduction using warp primitives
         */
        template <typename T>
        __device__ T block_reduce_min(T val) {
            static __shared__ T shared[32];

            int lane = threadIdx.x % 32;
            int warp_id = threadIdx.x / 32;

            val = warp_reduce_min(val);

            if (lane == 0) {
                shared[warp_id] = val;
            }
            __syncthreads();

            if (warp_id == 0) {
                val = (threadIdx.x < (blockDim.x + 31) / 32) ? shared[lane] : std::numeric_limits<T>::infinity();
                val = warp_reduce_min(val);
            }

            return val;
        }

        /**
         * @brief Block-level product reduction using warp primitives
         */
        template <typename T>
        __device__ T block_reduce_prod(T val) {
            static __shared__ T shared[32];

            int lane = threadIdx.x % 32;
            int warp_id = threadIdx.x / 32;

            val = warp_reduce_prod(val);

            if (lane == 0) {
                shared[warp_id] = val;
            }
            __syncthreads();

            if (warp_id == 0) {
                val = (threadIdx.x < (blockDim.x + 31) / 32) ? shared[lane] : T(1);
                val = warp_reduce_prod(val);
            }

            return val;
        }

        // ============= VECTORIZED LOADS + WARP REDUCTION =============

        /**
         * @brief Vectorized sum reduction using float4 loads + warp shuffles
         *
         * This combines two optimizations:
         * 1. Load 4 floats at once (4x memory bandwidth) - only if properly aligned
         * 2. Warp-level reduction (5-10x faster than shared memory)
         *
         * Expected speedup: 10-20x over naive implementation (when aligned)!
         *
         * @param input Input data (should be 16-byte aligned for best performance)
         * @param n Number of elements
         * @return Sum of all elements (valid only in thread 0 of each block)
         */
        template <typename T>
        __device__ T vectorized_block_reduce_sum(const T* input, size_t n) {
            size_t vec_idx = blockIdx.x * blockDim.x + threadIdx.x;
            size_t idx = vec_idx * 4;

            T val = 0;
            if constexpr (std::is_same_v<T, float>) {
                // Check alignment at runtime (pointer must be 16-byte aligned)
                bool is_aligned = (reinterpret_cast<uintptr_t>(input) % 16) == 0;

                if (is_aligned && idx + 3 < n) {
                    // Load 4 floats in one transaction (16 bytes aligned)
                    float4 vals = reinterpret_cast<const float4*>(input)[vec_idx];
                    val = vals.x + vals.y + vals.z + vals.w;
                } else if (idx < n) {
                    // Scalar fallback for unaligned data or remainder
                    for (size_t i = idx; i < n && i < idx + 4; ++i) {
                        val += input[i];
                    }
                }
            } else {
                // Fallback for non-float types
                if (idx < n) {
                    for (size_t i = idx; i < n && i < idx + 4; ++i) {
                        val += input[i];
                    }
                }
            }

            // Warp-level reduction
            return block_reduce_sum(val);
        }

        /**
         * @brief Vectorized max reduction using float4 loads + warp shuffles
         */
        template <typename T>
        __device__ T vectorized_block_reduce_max(const T* input, size_t n) {
            size_t vec_idx = blockIdx.x * blockDim.x + threadIdx.x;
            size_t idx = vec_idx * 4;

            T val = -std::numeric_limits<T>::infinity();
            if constexpr (std::is_same_v<T, float>) {
                bool is_aligned = (reinterpret_cast<uintptr_t>(input) % 16) == 0;

                if (is_aligned && idx + 3 < n) {
                    float4 vals = reinterpret_cast<const float4*>(input)[vec_idx];
                    val = fmaxf(fmaxf(vals.x, vals.y), fmaxf(vals.z, vals.w));
                } else if (idx < n) {
                    for (size_t i = idx; i < n && i < idx + 4; ++i) {
                        val = fmaxf(val, input[i]);
                    }
                }
            } else {
                if (idx < n) {
                    for (size_t i = idx; i < n && i < idx + 4; ++i) {
                        val = (input[i] > val) ? input[i] : val;
                    }
                }
            }

            return block_reduce_max(val);
        }

        /**
         * @brief Vectorized min reduction using float4 loads + warp shuffles
         */
        template <typename T>
        __device__ T vectorized_block_reduce_min(const T* input, size_t n) {
            size_t vec_idx = blockIdx.x * blockDim.x + threadIdx.x;
            size_t idx = vec_idx * 4;

            T val = std::numeric_limits<T>::infinity();
            if constexpr (std::is_same_v<T, float>) {
                bool is_aligned = (reinterpret_cast<uintptr_t>(input) % 16) == 0;

                if (is_aligned && idx + 3 < n) {
                    float4 vals = reinterpret_cast<const float4*>(input)[vec_idx];
                    val = fminf(fminf(vals.x, vals.y), fminf(vals.z, vals.w));
                } else if (idx < n) {
                    for (size_t i = idx; i < n && i < idx + 4; ++i) {
                        val = fminf(val, input[i]);
                    }
                }
            } else {
                if (idx < n) {
                    for (size_t i = idx; i < n && i < idx + 4; ++i) {
                        val = (input[i] < val) ? input[i] : val;
                    }
                }
            }

            return block_reduce_min(val);
        }

        /**
         * @brief Vectorized product reduction using float4 loads + warp shuffles
         */
        template <typename T>
        __device__ T vectorized_block_reduce_prod(const T* input, size_t n) {
            size_t vec_idx = blockIdx.x * blockDim.x + threadIdx.x;
            size_t idx = vec_idx * 4;

            T val = 1;
            if constexpr (std::is_same_v<T, float>) {
                bool is_aligned = (reinterpret_cast<uintptr_t>(input) % 16) == 0;

                if (is_aligned && idx + 3 < n) {
                    float4 vals = reinterpret_cast<const float4*>(input)[vec_idx];
                    val = vals.x * vals.y * vals.z * vals.w;
                } else if (idx < n) {
                    for (size_t i = idx; i < n && i < idx + 4; ++i) {
                        val *= input[i];
                    }
                }
            } else {
                if (idx < n) {
                    for (size_t i = idx; i < n && i < idx + 4; ++i) {
                        val *= input[i];
                    }
                }
            }

            return block_reduce_prod(val);
        }

        // ============= SEGMENTED REDUCTION HELPERS =============

        /**
         * @brief Vectorized segmented reduction using warp shuffles
         *
         * Reduces a single contiguous segment with vectorized loads + warp reduction.
         * Much faster than CUB's iterator-based approach for medium-sized segments.
         *
         * @tparam T Type of elements (float, int, etc.)
         * @param segment_start Pointer to start of segment
         * @param segment_size Number of elements in segment
         * @return Reduced value (valid only in thread 0)
         */
        template <typename T>
        __device__ T vectorized_segment_reduce_sum(const T* segment_start, size_t segment_size) {
            // Use double accumulation for float to avoid precision loss
            double val = 0.0;
            if constexpr (std::is_same_v<T, float>) {
                bool is_aligned = (reinterpret_cast<uintptr_t>(segment_start) % 16) == 0;

                // Grid-stride loop: Process entire segment across all threads
                size_t stride = blockDim.x * 4; // Each iteration processes blockDim.x * 4 elements
                for (size_t base = 0; base < segment_size; base += stride) {
                    size_t vec_idx = threadIdx.x;
                    size_t idx = base + vec_idx * 4;

                    // Vectorized load: 4 elements per thread (only if aligned)
                    if (is_aligned && idx + 3 < segment_size) {
                        float4 vals = reinterpret_cast<const float4*>(segment_start)[base / 4 + vec_idx];
                        val += (double)vals.x + (double)vals.y + (double)vals.z + (double)vals.w;
                    } else if (idx < segment_size) {
                        // Scalar fallback for unaligned data or remainder
                        for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                            val += (double)segment_start[i];
                        }
                    }
                }

                // Warp-level reduction with double precision, then cast back
                double sum = block_reduce_sum(val);
                return static_cast<T>(sum);
            } else {
                // Fallback for non-float types
                T int_val = 0;
                size_t stride = blockDim.x * 4;
                for (size_t base = 0; base < segment_size; base += stride) {
                    size_t vec_idx = threadIdx.x;
                    size_t idx = base + vec_idx * 4;

                    if (idx < segment_size) {
                        for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                            int_val += segment_start[i];
                        }
                    }
                }
                return block_reduce_sum(int_val);
            }
        }

        /**
         * @brief Vectorized segmented max reduction
         */
        template <typename T>
        __device__ T vectorized_segment_reduce_max(const T* segment_start, size_t segment_size) {
            T val = -std::numeric_limits<T>::infinity();
            if constexpr (std::is_same_v<T, float>) {
                bool is_aligned = (reinterpret_cast<uintptr_t>(segment_start) % 16) == 0;

                // Grid-stride loop: Process entire segment across all threads
                size_t stride = blockDim.x * 4;
                for (size_t base = 0; base < segment_size; base += stride) {
                    size_t vec_idx = threadIdx.x;
                    size_t idx = base + vec_idx * 4;

                    if (is_aligned && idx + 3 < segment_size) {
                        float4 vals = reinterpret_cast<const float4*>(segment_start)[base / 4 + vec_idx];
                        val = fmaxf(val, fmaxf(fmaxf(vals.x, vals.y), fmaxf(vals.z, vals.w)));
                    } else if (idx < segment_size) {
                        for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                            val = fmaxf(val, segment_start[i]);
                        }
                    }
                }
            } else {
                size_t stride = blockDim.x * 4;
                for (size_t base = 0; base < segment_size; base += stride) {
                    size_t vec_idx = threadIdx.x;
                    size_t idx = base + vec_idx * 4;

                    if (idx < segment_size) {
                        for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                            val = (segment_start[i] > val) ? segment_start[i] : val;
                        }
                    }
                }
            }

            return block_reduce_max(val);
        }

        /**
         * @brief Vectorized segmented min reduction
         */
        template <typename T>
        __device__ T vectorized_segment_reduce_min(const T* segment_start, size_t segment_size) {
            T val = std::numeric_limits<T>::infinity();
            if constexpr (std::is_same_v<T, float>) {
                bool is_aligned = (reinterpret_cast<uintptr_t>(segment_start) % 16) == 0;

                // Grid-stride loop: Process entire segment across all threads
                size_t stride = blockDim.x * 4;
                for (size_t base = 0; base < segment_size; base += stride) {
                    size_t vec_idx = threadIdx.x;
                    size_t idx = base + vec_idx * 4;

                    if (is_aligned && idx + 3 < segment_size) {
                        float4 vals = reinterpret_cast<const float4*>(segment_start)[base / 4 + vec_idx];
                        val = fminf(val, fminf(fminf(vals.x, vals.y), fminf(vals.z, vals.w)));
                    } else if (idx < segment_size) {
                        for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                            val = fminf(val, segment_start[i]);
                        }
                    }
                }
            } else {
                size_t stride = blockDim.x * 4;
                for (size_t base = 0; base < segment_size; base += stride) {
                    size_t vec_idx = threadIdx.x;
                    size_t idx = base + vec_idx * 4;

                    if (idx < segment_size) {
                        for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                            val = (segment_start[i] < val) ? segment_start[i] : val;
                        }
                    }
                }
            }

            return block_reduce_min(val);
        }

        template <typename T>
        __device__ T vectorized_segment_reduce_prod(const T* segment_start, size_t segment_size) {
            T val = T(1);
            if constexpr (std::is_same_v<T, float>) {
                bool is_aligned = (reinterpret_cast<uintptr_t>(segment_start) % 16) == 0;

                size_t stride = blockDim.x * 4;
                for (size_t base = 0; base < segment_size; base += stride) {
                    size_t vec_idx = threadIdx.x;
                    size_t idx = base + vec_idx * 4;

                    if (is_aligned && idx + 3 < segment_size) {
                        float4 vals = reinterpret_cast<const float4*>(segment_start)[base / 4 + vec_idx];
                        val *= vals.x * vals.y * vals.z * vals.w;
                    } else if (idx < segment_size) {
                        for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                            val *= segment_start[i];
                        }
                    }
                }
            } else {
                size_t stride = blockDim.x * 4;
                for (size_t base = 0; base < segment_size; base += stride) {
                    size_t vec_idx = threadIdx.x;
                    size_t idx = base + vec_idx * 4;

                    if (idx < segment_size) {
                        for (size_t i = idx; i < segment_size && i < idx + 4; ++i) {
                            val *= segment_start[i];
                        }
                    }
                }
            }

            return block_reduce_prod(val);
        }

    } // namespace warp_ops
} // namespace lfs::core
